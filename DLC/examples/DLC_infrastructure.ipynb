{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the deeplabcut infrastructure modules\n",
    "This is a notebook where I am going through the code in DLC to understand it. We will use the Reaching Mackenzie demo data (provided with DeepLabCut). \n",
    "\n",
    "* Load modules\n",
    "* Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.4.1\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "print(deeplabcut.__version__)\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_dir: /home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples\n",
      "/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/config.yaml\n",
      "/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/pose_cfg.yaml\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ipython/ipython/issues/10123#issuecomment-354889020\n",
    "if not 'nb_dir' in globals():\n",
    "    nb_dir = os.getcwd()\n",
    "print('nb_dir: ' + nb_dir)\n",
    "os.chdir(nb_dir)\n",
    "\n",
    "exp_id = \"Demo\"\n",
    "experiment = {\"Demo\": [\"Reaching-Mackenzie-2018-08-30\", \"ReachingAug30-trainset95shuffle1\"],\n",
    "              \"FSL\": [\"Fly-Sayed-2019-02-20\", \"FlyFeb20-trainset95shuffle1\"]}\n",
    "\n",
    "experiment = experiment[exp_id]\n",
    "main_path=os.path.join(nb_dir, experiment[0])\n",
    "config_path = os.path.join(main_path, \"config.yaml\")\n",
    "poseconfigfile=Path(os.path.join(main_path, \"dlc-models/iteration-0/\", experiment[1], \"train/pose_cfg.yaml\"))\n",
    "config_yaml = poseconfigfile\n",
    "print(config_path)\n",
    "print(config_yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `train()` function in `pose_estimation_tensorflow/train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeplabcut.train_network() is just a CLI interface for the most common call to train(). Let us go deep into its code in order to understand it!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare variables inside the function scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-97ad8dade032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_yaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#switch to folder of config_yaml (for logging)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_estimation_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# def setup_logging():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train'"
     ]
    }
   ],
   "source": [
    "shuffle=1\n",
    "trainingsetindex=0\n",
    "# gputouse=None\n",
    "max_snapshots_to_keep=5\n",
    "max_to_keep=max_snapshots_to_keep\n",
    "autotune=False\n",
    "\n",
    "### Function arguments\n",
    "displayiters=10\n",
    "display_iters=displayiters\n",
    "# saveiters=1000\n",
    "saveiters=100\n",
    "save_iters=saveiters\n",
    "# maxiters=200000\n",
    "maxiters=100\n",
    "\n",
    "max_iter=maxiters\n",
    "\n",
    "import logging, os\n",
    "os.chdir(str(Path(config_yaml).parents[0])) #switch to folder of config_yaml (for logging)\n",
    "from deeplabcut.pose_estimation_tensorflow.util.logging import setup_logging\n",
    "# def setup_logging():\n",
    "#     FORMAT = '%(asctime)-15s %(message)s'\n",
    "#     logging.basicConfig(filename=os.path.join('log.txt'), filemode='w',\n",
    "#                         datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#                         level=logging.INFO, format=FORMAT)\n",
    "\n",
    "#     console = logging.StreamHandler()\n",
    "#     console.setLevel(logging.INFO)\n",
    "#     logging.getLogger('').addHandler(console)\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/pose_cfg.yaml')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_path=os.getcwd()\n",
    "print(start_path)\n",
    "Path(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is cfg created in train() @ pose_estimation_tensorflow/train.py?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0]],\n",
      " 'all_joints_names': ['Separator'],\n",
      " 'batch_size': 1,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FlyFeb20/Fly_Sayed95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FlyFeb20/Documentation_data-Fly_95shuffle1.pickle',\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 1,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Fly-Sayed-2019-02-20',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Fly-Sayed-2019-02-20/dlc-models/iteration-0/FlyFeb20-trainset95shuffle1/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'use_gt_segm': False,\n",
      " 'video': False,\n",
      " 'video_batch': False,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "cfg = deeplabcut.pose_estimation_tensorflow.config.load_config(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cfg is the output of `pose_estimation_tensorflow.config.load_config(path/config.yaml)`\n",
    "2. `load_config()` is a call to `pose_estimation_tensorflow.config.cfg_from_file()` with the only argument it has set to: `\"pose_cfg.yaml\"`\n",
    "3. `cfg_from_file`:\n",
    "  * reads the pose_config.yaml file into an EasyDict\n",
    "  * sets the snapshot_prefix to a default (depends on the trainpath, not default)\n",
    "  * merges the pose_config with the default configurations (extracted via `cfg = auxiliaryfunctions.read_config(config)`)\n",
    "  * merging is done via `pose_estimation_tensorflow.config._merge_a_into_b(a, b)`, a recursive function that calls itself!\n",
    "\n",
    "4. Again in train(), the batch_size is set to a default of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the dataset object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset has 18 images\n"
     ]
    }
   ],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.dataset.factory import create as create_dataset\n",
    "dataset = create_dataset(cfg)\n",
    "print(\"The current dataset has {} images\".format(dataset.num_images))\n",
    "\n",
    "i=0\n",
    "dataset_array=dataset.raw_data[\"dataset\"]\n",
    "sample=dataset_array[0,i]\n",
    "# len(sample)\n",
    "# print(sample[0][0]) # path\n",
    "# sample[1][0] # size [channel, h, w]\n",
    "# joints = sample[2][0][0] # features [[id, x, y],\n",
    "#                          #           [id, x, y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([[  0, 191,  41]])]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.raw_data[\"dataset\"][:,0][0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiates a `PoseDataset` object with the configuration supplied in cfg\n",
    "It will have\n",
    "1. Metadata attributes describing the dataset\n",
    "2. A raw_data attribute with metadata and another attribute called dataset. This contains metadata about the images!\n",
    "  * It's an array of size (1, num_images)\n",
    "  * Each ith element is a list like object (np.void) for the ith image\n",
    "  * [0][0] gives the path to the image\n",
    "  * [0][1] is the size (channels, h, w)\n",
    "  * [0][2] is the features (joints) in an array of shape nx3 where n is the number of features (feature_id, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is batch_spec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.nnet.pose_net import get_batch_spec\n",
    "\n",
    "# from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import Batch\n",
    "# class Batch(Enum):\n",
    "#     inputs = 0\n",
    "#     part_score_targets = 1\n",
    "#     part_score_weights = 2\n",
    "#     locref_targets = 3\n",
    "#     locref_mask = 4\n",
    "#     data_item = 5\n",
    "\n",
    "    \n",
    "# def get_batch_spec(cfg):\n",
    "#     num_joints = cfg.num_joints\n",
    "#     batch_size = cfg.batch_size\n",
    "#     return {\n",
    "#         Batch.inputs: [batch_size, None, None, 3],\n",
    "#         Batch.part_score_targets: [batch_size, None, None, num_joints],\n",
    "#         Batch.part_score_weights: [batch_size, None, None, num_joints],\n",
    "#         Batch.locref_targets: [batch_size, None, None, num_joints * 2],\n",
    "#         Batch.locref_mask: [batch_size, None, None, num_joints * 2]\n",
    "#     }\n",
    "\n",
    "batch_spec = get_batch_spec(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*batch* *specifications*: Consists of a dictionary with 5 entries, each storing a list of length 4.\n",
    "The keys in this dictionary are Enum objects defined in:\n",
    "\n",
    "`from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import Batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Batch.inputs: 0>: [1, None, None, 3],\n",
       " <Batch.part_score_targets: 1>: [1, None, None, 1],\n",
       " <Batch.part_score_weights: 2>: [1, None, None, 1],\n",
       " <Batch.locref_targets: 3>: [1, None, None, 2],\n",
       " <Batch.locref_mask: 4>: [1, None, None, 2]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is batch, enqueue_op and placeholders (output of setup_preloading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.train import setup_preloading\n",
    "# def setup_preloading(batch_spec):\n",
    "#     placeholders = {name: tf.placeholder(tf.float32, shape=spec) for (name, spec) in batch_spec.items()}\n",
    "#     names = placeholders.keys()\n",
    "#     placeholders_list = list(placeholders.values())\n",
    "\n",
    "#     QUEUE_SIZE = 20\n",
    "\n",
    "#     q = tf.FIFOQueue(QUEUE_SIZE, [tf.float32]*len(batch_spec))\n",
    "#     enqueue_op = q.enqueue(placeholders_list)\n",
    "#     batch_list = q.dequeue()\n",
    "\n",
    "#     batch = {}\n",
    "#     for idx, name in enumerate(names):\n",
    "#         batch[name] = batch_list[idx]\n",
    "#         batch[name].set_shape(batch_spec[name])\n",
    "#     return batch, enqueue_op, placeholders\n",
    "batch, enqueue_op, placeholders = setup_preloading(batch_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**batch** is a dictionary analogous to batch_spec that contains tf.tensors of class `tf.Tensor 'fifo_queue_1_Dequeue:i'`. Length = to the lists in batch_spec. float32.\n",
    "dequeue tensors consist of the data collected after a dequeueing process in a TF queue. TF queues are the way TF provides support for multithreading (parallel) computing. It's an abstraction representing data tensors that support multithreading, but they can be treated as data or data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Batch.inputs: 0>: <tf.Tensor 'fifo_queue_Dequeue:0' shape=(1, ?, ?, 3) dtype=float32>,\n",
       " <Batch.part_score_targets: 1>: <tf.Tensor 'fifo_queue_Dequeue:1' shape=(1, ?, ?, 1) dtype=float32>,\n",
       " <Batch.part_score_weights: 2>: <tf.Tensor 'fifo_queue_Dequeue:2' shape=(1, ?, ?, 1) dtype=float32>,\n",
       " <Batch.locref_targets: 3>: <tf.Tensor 'fifo_queue_Dequeue:3' shape=(1, ?, ?, 2) dtype=float32>,\n",
       " <Batch.locref_mask: 4>: <tf.Tensor 'fifo_queue_Dequeue:4' shape=(1, ?, ?, 2) dtype=float32>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**enqueue_op** is a tf.Operation object:\n",
    "`<tf.Operation 'fifo_queue_1_enqueue' type=QueueEnqueueV2>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**placeholders** is a dictionary like **batch** storing tensors of class a `tf.Tensor 'Placeholder_5:0'`. Length the same and float 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Batch.inputs: 0>: <tf.Tensor 'Placeholder:0' shape=(1, ?, ?, 3) dtype=float32>,\n",
       " <Batch.part_score_targets: 1>: <tf.Tensor 'Placeholder_1:0' shape=(1, ?, ?, 1) dtype=float32>,\n",
       " <Batch.part_score_weights: 2>: <tf.Tensor 'Placeholder_2:0' shape=(1, ?, ?, 1) dtype=float32>,\n",
       " <Batch.locref_targets: 3>: <tf.Tensor 'Placeholder_3:0' shape=(1, ?, ?, 2) dtype=float32>,\n",
       " <Batch.locref_mask: 4>: <tf.Tensor 'Placeholder_4:0' shape=(1, ?, ?, 2) dtype=float32>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up: batch, batch_spec and placeholders are dictionaries with 5 entries. Every key is a Batch object (Enum class). The 5 keys are shared by all 3 dictionaries.\n",
    "* batch stores **TF queue_Dequeue tensors**\n",
    "* batch_spec stores **Python lists**\n",
    "* placeholders stores **TF Placeholder tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the 5 keys??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is losses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses is a dict with three different losses, each a tensor\n",
    "1. **part_loss**: `<tf.Tensor 'sigmoid_cross_entropy_loss/value:0' shape=() dtype=float32>`\n",
    "2. **locref_loss**: `<tf.Tensor 'mul:0' shape=() dtype=float32>`\n",
    "3. **total_loss**: `<tf.Tensor 'add:0' shape=() dtype=float32>`\n",
    "\n",
    "It's the output of the `train` method in the PoseNet class.\n",
    "  * `PoseNet.train(batch)\n",
    "  \n",
    "Their value is tracked with the `summary.scalars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fifo_queue_Dequeue:2\", shape=(1, ?, ?, 1), dtype=float32)\n",
      "dict_keys([<Batch.inputs: 0>, <Batch.part_score_targets: 1>, <Batch.part_score_weights: 2>, <Batch.locref_targets: 3>, <Batch.locref_mask: 4>])\n"
     ]
    }
   ],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import Batch\n",
    "print(batch[Batch.part_score_weights])\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.nnet.net_factory import pose_net\n",
    "myobject = pose_net(cfg)\n",
    "losses = myobject.train(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking deeper into the pose_net(cfg).train(batch) statement\n",
    "This statement does the following:\n",
    "\n",
    "* Instantiate an object of class PoseNet taking the cfg file as input. cfg is assigned to self\n",
    "* Then, the train method in the object is called with batch as input\n",
    "\n",
    "**The train() method** runs the get_net() method with the value under Batch.input key in the batch dictionary (more details in comments).\n",
    "This gives us heads, which is a dictionary with 2 keys, `part_pred` and `locref` (1). The values are `slim.conv2d_transpose` objects, the output of `prediction_layer` i.e. they represent prediction layers, one for each different loss.\n",
    "The resut of the function computes the loss based on the said prediction layers contained in heads.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. `part_pred_interm` is a third key available only if `location_refinement` is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(cfg.location_refinement)\n",
    "print(cfg.intermediate_supervision)\n",
    "print(cfg.locref_huber_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_net(self, inputs):\n",
    "#     # get_net calls extract_features and prediction layers\n",
    "#     net, end_points = self.extract_features(inputs)\n",
    "#     return self.prediction_layers(net, end_points)\n",
    "\n",
    "def train(self, batch):\n",
    "    cfg = self.cfg\n",
    "\n",
    "    # call to the get_net() function defined above\n",
    "    # * loads the resnet50 function (based on the cfg params)\n",
    "    # * performs a substraction on the mean of the images\n",
    "    # * adds prediction(s) layer.\n",
    "    #    * The size of the layer depends on the number of features tracked.\n",
    "    #    *  The number of prediction layers depends on the type of losses used (up to three different ones)\n",
    "    \n",
    "    # This is achieved with 2 functions: extract_features() and prediction_layers()\n",
    "    # * extract_features() is just a customised call to resnet_v1 that\n",
    "    #   - makes easy selection of the right resnet (50, 100, ...)\n",
    "    #   - substracts the mean as stated above\n",
    "    #   - modifies the default arguments of the resnet generator:\n",
    "    #     global_pool=False, output_stride=16,is_training=False.\n",
    "    # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py\n",
    "    \n",
    "    # * prediction_layers()\n",
    "    #   - calls prediction_layer() for every loss being computed\n",
    "    #   - fine tunes the call to prediction layer depending on the number of outputs \n",
    "    #     in part_pred loss, n_outs = n_features tracked.\n",
    "    #     in locref, n_outs = n_features tracked * 2   \n",
    "    heads = self.get_net(batch[Batch.inputs])\n",
    "    # the output is a dictionary with every key being a prediction layer i.e. the very tip of the TF graph\n",
    "\n",
    "    weigh_part_predictions = cfg.weigh_part_predictions\n",
    "    part_score_weights = batch[Batch.part_score_weights] if weigh_part_predictions else 1.0\n",
    "\n",
    "    def add_part_loss(pred_layer):\n",
    "        return tf.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets], # ground truth (labels/targets)\n",
    "                                                   heads[pred_layer],           # prediction\n",
    "                                                   part_score_weights)\n",
    "\n",
    "    loss = {}\n",
    "    loss['part_loss'] = add_part_loss('part_pred')\n",
    "    total_loss = loss['part_loss']\n",
    "    # intermediate_supervision (I don't know yet) is False in the reaching mackenzie example\n",
    "    if cfg.intermediate_supervision:\n",
    "        loss['part_loss_interm'] = add_part_loss('part_pred_interm')\n",
    "        total_loss = total_loss + loss['part_loss_interm']\n",
    "\n",
    "        \n",
    "    # location_refinement (i.e. location of the features) is True in the reaching mackenzie example\n",
    "    if cfg.location_refinement:\n",
    "        locref_pred = heads['locref']\n",
    "        locref_targets = batch[Batch.locref_targets]\n",
    "        locref_weights = batch[Batch.locref_mask]\n",
    "\n",
    "        loss_func = losses.huber_loss if cfg.locref_huber_loss else tf.losses.mean_squared_error\n",
    "        loss['locref_loss'] = cfg.locref_loss_weight * loss_func(locref_targets, locref_pred, locref_weights)\n",
    "        total_loss = total_loss + loss['locref_loss']\n",
    "\n",
    "        # loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize)\n",
    "    loss['total_loss'] = total_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def extract_features(self, inputs):\n",
    "        net_fun = net_funcs[self.cfg.net_type]\n",
    "\n",
    "        mean = tf.constant(self.cfg.mean_pixel,\n",
    "                           dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "        im_centered = inputs - mean\n",
    "\n",
    "        # The next part of the code depends upon which tensorflow version you have.\n",
    "        vers = tf.__version__\n",
    "        vers = vers.split(\".\") #Updated based on https://github.com/AlexEMG/DeepLabCut/issues/44\n",
    "        if int(vers[0])==1 and int(vers[1])<4: #check if lower than version 1.4.\n",
    "            with slim.arg_scope(resnet_v1.resnet_arg_scope(False)):\n",
    "                net, end_points = net_fun(im_centered,\n",
    "                                          global_pool=False, output_stride=16)\n",
    "        else:\n",
    "            with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "                net, end_points = net_fun(im_centered,\n",
    "                                          global_pool=False, output_stride=16,is_training=False)\n",
    "\n",
    "        return net,end_points\n",
    "\n",
    "def prediction_layers(self, features, end_points, reuse=None):\n",
    "    cfg = self.cfg\n",
    "\n",
    "    num_layers = re.findall(\"resnet_([0-9]*)\", cfg.net_type)[0]\n",
    "    layer_name = 'resnet_v1_{}'.format(num_layers) + '/block{}/unit_{}/bottleneck_v1'\n",
    "\n",
    "    out = {}\n",
    "    with tf.variable_scope('pose', reuse=reuse):\n",
    "        out['part_pred'] = prediction_layer(cfg, features, 'part_pred',\n",
    "                                            cfg.num_joints)\n",
    "        if cfg.location_refinement:\n",
    "            out['locref'] = prediction_layer(cfg, features, 'locref_pred',\n",
    "                                             cfg.num_joints * 2)\n",
    "        if cfg.intermediate_supervision:\n",
    "            interm_name = layer_name.format(3, cfg.intermediate_supervision_layer)\n",
    "            block_interm_out = end_points[interm_name]\n",
    "            out['part_pred_interm'] = prediction_layer(cfg, block_interm_out,\n",
    "                                                       'intermediate_supervision',\n",
    "                                                       cfg.num_joints)\n",
    "\n",
    "    return out\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# losses = pose_net(cfg).train(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the loss is computed..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the losses to the TF tracking system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, t in losses.items():\n",
    "    tf.summary.scalar(k, t)\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "total_loss = losses[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Variables to restore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "variables_to_restore = slim.get_variables_to_restore(include=[\"resnet_v1\"])\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "saver = tf.train.Saver(max_to_keep=max_to_keep)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge amount of variables extracted from the pretrained ResNet50 print(len(variables_to_restore))\n",
    "TODO: Google restorers and savers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/conv1/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/weights:0' shape=(1, 1, 64, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights:0' shape=(1, 1, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/weights:0' shape=(1, 1, 64, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/weights:0' shape=(1, 1, 256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/weights:0' shape=(1, 1, 64, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/weights:0' shape=(1, 1, 256, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/weights:0' shape=(1, 1, 64, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/weights:0' shape=(1, 1, 256, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/weights:0' shape=(1, 1, 256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/weights:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/weights:0' shape=(1, 1, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/weights:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/weights:0' shape=(1, 1, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/weights:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/weights:0' shape=(1, 1, 512, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/weights:0' shape=(1, 1, 128, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/weights:0' shape=(1, 1, 512, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/weights:0' shape=(1, 1, 512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/weights:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/weights:0' shape=(1, 1, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/weights:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/weights:0' shape=(1, 1, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/weights:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/weights:0' shape=(1, 1, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/weights:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/weights:0' shape=(1, 1, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/weights:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/weights:0' shape=(1, 1, 1024, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/weights:0' shape=(1, 1, 256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/weights:0' shape=(1, 1, 1024, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/weights:0' shape=(1, 1, 1024, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/weights:0' shape=(1, 1, 512, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/weights:0' shape=(1, 1, 2048, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/weights:0' shape=(1, 1, 512, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/weights:0' shape=(1, 1, 2048, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/gamma:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/beta:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/weights:0' shape=(1, 1, 512, 2048) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/gamma:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/beta:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean:0' shape=(2048,) dtype=float32_ref>,\n",
       " <tf.Variable 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance:0' shape=(2048,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(max_to_keep)\n",
    "variables_to_restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a session and preload it\n",
    "i.e. give it instructions on how to load data\n",
    "\n",
    "What it does:\n",
    "\n",
    "1. Instantiates a Coordinator object, which ensures that all parallel threads are ended simultaneously and merges them to have an integrated output: https://adventuresinmachinelearning.com/introduction-tensorflow-queuing/\n",
    "\n",
    "2. Declares the threading process, which is defined by load_and_enqueue():\n",
    "\n",
    "```\n",
    "def load_and_enqueue(sess, enqueue_op, coord, dataset, placeholders):\n",
    "    while not coord.should_stop():\n",
    "        batch_np = dataset.next_batch()\n",
    "        food = {pl: batch_np[name] for (name, pl) in placeholders.items()}\n",
    "        sess.run(enqueue_op, feed_dict=food)\n",
    "```\n",
    "\n",
    "`load_and_enqueue()` runs the enqueue operation (enqueue_op) using food. Food is a dictionary where every entry is a a placeholder (pl) from the placeholders dict. The placeholders are loaded with the data from one batch. The function thus\n",
    "1. Retrieves the next batch (batch_np) from the dataset object, as long as the coordinator does not signal the end of the computations.\n",
    "2. Fills the placeholders with the data of that batch and puts them in a new dict food\n",
    "3. Enqueue the data of the batch in the TF queueing sytem\n",
    "\n",
    "This mechanism is wrapped by the `Thread()` class, which makes every thread run the operations defined in `load_and_enqueue()` in parallel.\n",
    "The input arguments of `start_preloading()` are passed untouched to every thread to do with them as explained above.\n",
    "\n",
    "```\n",
    "def start_preloading(sess, enqueue_op, dataset, placeholders):\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    t = threading.Thread(target=load_and_enqueue,\n",
    "                         args=(sess, enqueue_op, coord, dataset, placeholders))\n",
    "    t.start()\n",
    "\n",
    "    return coord, t\n",
    "```\n",
    "\n",
    "In a nutshell, as long as the threads are not to be stopped, the next batch is loaded, and the placeholders defined above are loaded with data. Finally, they are passed to the enqueue operation to be run by the TF session.\n",
    "\n",
    "3. Starts the threading process\n",
    "\n",
    "This returns the coordinator and thread objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "\n",
    "# from deeplabcut.pose_estimation_tensorflow.train import load_and_enqueue      \n",
    "from deeplabcut.pose_estimation_tensorflow.train import start_preloading\n",
    "coord, thread = start_preloading(sess, enqueue_op, dataset, placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare a loss optimization method (specified in the cfg file). Options are SGD or Adam\n",
    "\n",
    "The total loss will be optimized. The total loss is the sum of all the computed losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.train import get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(cfg.log_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, train_op = get_optimizer(total_loss, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from /home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_iters overwritten as 100\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 100\n"
     ]
    }
   ],
   "source": [
    "# Restore variables from disk.\n",
    "restorer.restore(sess, cfg.init_weights)\n",
    "if maxiters==None:\n",
    "    max_iter = int(cfg.multi_step[-1][1])\n",
    "else:\n",
    "    max_iter = min(int(cfg.multi_step[-1][1]),int(maxiters))\n",
    "    #display_iters = max(1,int(displayiters))\n",
    "    print(\"Max_iters overwritten as\",max_iter)\n",
    "\n",
    "if displayiters==None:\n",
    "    display_iters = max(1,int(cfg.display_iters))\n",
    "else:\n",
    "    display_iters = max(1,int(displayiters))\n",
    "    print(\"Display_iters overwritten as\",display_iters)\n",
    "\n",
    "if saveiters==None:\n",
    "     save_iters=max(1,int(cfg.save_iters))\n",
    "\n",
    "else:\n",
    "    save_iters=max(1,int(saveiters))\n",
    "    print(\"Save_iters overwritten as\",save_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.train import LearningRate\n",
    "\n",
    "# class LearningRate(object):\n",
    "#     def __init__(self, cfg):\n",
    "#         self.steps = cfg.multi_step\n",
    "#         self.current_step = 0\n",
    "\n",
    "#     def get_lr(self, iteration):\n",
    "#         lr = self.steps[self.current_step][0]\n",
    "#         if iteration == self.steps[self.current_step][1]:\n",
    "#             self.current_step += 1\n",
    "\n",
    "#         return lr\n",
    "\n",
    "    \n",
    "cum_loss = 0.0\n",
    "lr_gen = LearningRate(cfg)\n",
    "stats_path = Path(config_yaml).with_name('learning_stats.csv')\n",
    "lrf = open(str(stats_path), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deeplabcut.pose_estimation_tensorflow.train.LearningRate at 0x7f57b45801d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'dataset_type': 'default', 'use_gt_segm': False, 'batch_size': 1, 'video': False, 'video_batch': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['Hand', 'Finger1', 'Finger2', 'Joystick'], 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ReachingAug30/Reaching_Mackenzie95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ReachingAug30/Documentation_data-Reaching_95shuffle1.pickle', 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25}\n",
      "Starting training....\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1c0a305c34c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n\u001b[0;32m----> 8\u001b[0;31m                                       feed_dict={learning_rate: current_lr})\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "print(\"Training parameter:\")\n",
    "print(cfg)\n",
    "print(\"Starting training....\")\n",
    "for it in range(max_iter+1):\n",
    "    print(it)\n",
    "    current_lr = lr_gen.get_lr(it)\n",
    "    [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n",
    "                                      feed_dict={learning_rate: current_lr})\n",
    "    cum_loss += loss_val\n",
    "    train_writer.add_summary(summary, it)\n",
    "  \n",
    "    if it % display_iters == 0 and it>0:\n",
    "        average_loss = cum_loss / display_iters\n",
    "        cum_loss = 0.0\n",
    "        logging.info(\"iteration: {} loss: {} lr: {}\"\n",
    "                     .format(it, \"{0:.4f}\".format(average_loss), current_lr))\n",
    "        lrf.write(\"{}, {:.5f}, {}\\n\".format(it, average_loss, current_lr))\n",
    "        lrf.flush()\n",
    "\n",
    "    # Save snapshot\n",
    "    if (it % save_iters == 0 and it != 0) or it == max_iter:\n",
    "        model_name = cfg.snapshot_prefix\n",
    "        saver.save(sess, model_name, global_step=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[Node: fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/deeplabcut/pose_estimation_tensorflow/train.py\", line 53, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[Node: fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "Caused by op 'fifo_queue_enqueue', defined at:\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n",
      "    runner = Runner(result, future, yielded)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n",
      "    self.run()\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n",
      "    if (yield from self.run_code(code, result)):\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-4c8248b866f1>\", line 18, in <module>\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/deeplabcut/pose_estimation_tensorflow/train.py\", line 39, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 339, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3977, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[Node: fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrf.close()\n",
    "sess.close()\n",
    "writer.close()\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join([thread])\n",
    "#return to original path.\n",
    "os.chdir(str(start_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the autotune setting in TF 1.10.0\n",
    "Pending"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
