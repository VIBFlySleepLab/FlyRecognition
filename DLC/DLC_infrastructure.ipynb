{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the deeplabcut infrastructure modules\n",
    "This is a notebook where I am going through the code in DLC to understand it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "from pathlib import Path\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the autotune setting in TF 1.10.0\n",
    "Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/pose_cfg.yaml\n"
     ]
    }
   ],
   "source": [
    "# config_path = \"/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples/Arena-El-Sayed-2019-02-03/config.yaml\"\n",
    "# poseconfigfile=Path(\"/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples/Arena-El-Sayed-2019-02-03/dlc-models/iteration-0/ArenaFeb3-trainset95shuffle1/train/pose_cfg.yaml\")\n",
    "DLC_path=!pwd\n",
    "DLC_path=DLC_path[0]\n",
    "\n",
    "# DLC_path = \"/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC\"\n",
    "\n",
    "main_path=os.path.join(DLC_path, \"examples/Reaching-Mackenzie-2018-08-30\")\n",
    "config_path = os.path.join(main_path, \"config.yaml\")\n",
    "poseconfigfile=Path(os.path.join(main_path, \"dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/pose_cfg.yaml\"))\n",
    "config_yaml = poseconfigfile\n",
    "print(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `train()` function in `pose_estimation_tensorflow/train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeplabcut.train_network() is just a CLI interface for the most common call to train(). Let us go deep into its code in order to understand it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle=1\n",
    "trainingsetindex=0\n",
    "gputouse=None\n",
    "max_snapshots_to_keep=5\n",
    "max_to_keep=max_snapshots_to_keep\n",
    "utotune=False\n",
    "displayiters=100\n",
    "display_iters=displayiters\n",
    "# saveiters=1000\n",
    "saveiters=100\n",
    "save_iters=saveiters\n",
    "# maxiters=200000\n",
    "maxiters=100\n",
    "\n",
    "max_iter=maxiters\n",
    "\n",
    "import logging, os\n",
    "start_path=os.getcwd()\n",
    "os.chdir(str(Path(config_yaml).parents[0])) #switch to folder of config_yaml (for logging)\n",
    "from deeplabcut.pose_estimation_tensorflow.util.logging import setup_logging\n",
    "# def setup_logging():\n",
    "#     FORMAT = '%(asctime)-15s %(message)s'\n",
    "#     logging.basicConfig(filename=os.path.join('log.txt'), filemode='w',\n",
    "#                         datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#                         level=logging.INFO, format=FORMAT)\n",
    "\n",
    "#     console = logging.StreamHandler()\n",
    "#     console.setLevel(logging.INFO)\n",
    "#     logging.getLogger('').addHandler(console)\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is cfg created in train() @ pose_estimation_tensorflow/train.py?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['Hand', 'Finger1', 'Finger2', 'Joystick'],\n",
      " 'batch_size': 1,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ReachingAug30/Reaching_Mackenzie95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ReachingAug30/Documentation_data-Reaching_95shuffle1.pickle',\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples/Reaching-Mackenzie-2018-08-30',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/antortjim/MEGA/FlySleepLab/FlyRecognition/DLC/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'use_gt_segm': False,\n",
      " 'video': False,\n",
      " 'video_batch': False,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "cfg = deeplabcut.pose_estimation_tensorflow.config.load_config(poseconfigfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cfg is the output of `pose_estimation_tensorflow.config.load_config(path/poseconfigfile)`\n",
    "2. `load_config()` is a call to `pose_estimation_tensorflow.config.cfg_from_file()` with the only argument it has set to: `\"pose_cfg.yaml\"`\n",
    "3. `cfg_from_file`:\n",
    "  * reads the pose_config.yaml file into an EasyDict\n",
    "  * sets the snapshot_prefix to a default (depends on the trainpath, not default)\n",
    "  * merges the pose_config with the default configurations (extracted via `cfg = auxiliaryfunctions.read_config(config)`)\n",
    "  * merging is done via `pose_estimation_tensorflow.config._merge_a_into_b(a, b)`, a recursive function that calls itself!\n",
    "\n",
    "4. Again in train(), the batch_size is set to a default of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the dataset object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset has 18 images\n"
     ]
    }
   ],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.dataset.factory import create as create_dataset\n",
    "dataset = create_dataset(cfg)\n",
    "print(\"The current dataset has {} images\".format(dataset.num_images))\n",
    "\n",
    "i=0\n",
    "dataset_array=dataset.raw_data[\"dataset\"]\n",
    "sample=dataset_array[0,i]\n",
    "# len(sample)\n",
    "# print(sample[0][0]) # path\n",
    "# sample[1][0] # size [channel, h, w]\n",
    "# joints = sample[2][0][0] # features [[id, x, y],\n",
    "#                          #           [id, x, y]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiates a `PoseDataset` object with the configuration supplied in cfg\n",
    "It will have\n",
    "1. Metadata attributes describing the dataset\n",
    "2. A raw_data attribute with metadata and another attribute called dataset. This contains metadata about the images!\n",
    "  * It's an array of size (1, num_images)\n",
    "  * Each ith element is a list like object (np.void) for the ith image\n",
    "  * [0][0] gives the path to the image\n",
    "  * [1][0] is the size (channels, h, w)\n",
    "  * [2][0] is the features (joints) in an array of shape nx3 where n is the number of features (feature_id, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is batch_spec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.nnet.pose_net import get_batch_spec\n",
    "\n",
    "from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import Batch\n",
    "# class Batch(Enum):\n",
    "#     inputs = 0\n",
    "#     part_score_targets = 1\n",
    "#     part_score_weights = 2\n",
    "#     locref_targets = 3\n",
    "#     locref_mask = 4\n",
    "#     data_item = 5\n",
    "\n",
    "    \n",
    "# def get_batch_spec(cfg):\n",
    "#     num_joints = cfg.num_joints\n",
    "#     batch_size = cfg.batch_size\n",
    "#     return {\n",
    "#         Batch.inputs: [batch_size, None, None, 3],\n",
    "#         Batch.part_score_targets: [batch_size, None, None, num_joints],\n",
    "#         Batch.part_score_weights: [batch_size, None, None, num_joints],\n",
    "#         Batch.locref_targets: [batch_size, None, None, num_joints * 2],\n",
    "#         Batch.locref_mask: [batch_size, None, None, num_joints * 2]\n",
    "#     }\n",
    "\n",
    "batch_spec = get_batch_spec(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consists of a dictionary with 5 entries, each storing a list of length 4.\n",
    "The keys in this dictionary are Enum objects defined in:\n",
    "\n",
    "`from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import Batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Batch.inputs: 0>: [1, None, None, 3],\n",
       " <Batch.part_score_targets: 1>: [1, None, None, 4],\n",
       " <Batch.part_score_weights: 2>: [1, None, None, 4],\n",
       " <Batch.locref_targets: 3>: [1, None, None, 8],\n",
       " <Batch.locref_mask: 4>: [1, None, None, 8]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is batch, enqueue_op and placeholders (output of setup_preloading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.train import setup_preloading\n",
    "# def setup_preloading(batch_spec):\n",
    "#     placeholders = {name: tf.placeholder(tf.float32, shape=spec) for (name, spec) in batch_spec.items()}\n",
    "#     names = placeholders.keys()\n",
    "#     placeholders_list = list(placeholders.values())\n",
    "\n",
    "#     QUEUE_SIZE = 20\n",
    "\n",
    "#     q = tf.FIFOQueue(QUEUE_SIZE, [tf.float32]*len(batch_spec))\n",
    "#     enqueue_op = q.enqueue(placeholders_list)\n",
    "#     batch_list = q.dequeue()\n",
    "\n",
    "#     batch = {}\n",
    "#     for idx, name in enumerate(names):\n",
    "#         batch[name] = batch_list[idx]\n",
    "#         batch[name].set_shape(batch_spec[name])\n",
    "#     return batch, enqueue_op, placeholders\n",
    "batch, enqueue_op, placeholders = setup_preloading(batch_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**batch** is a dictionary analogous to batch_spec that contains tf.tensors of class `tf.Tensor 'fifo_queue_1_Dequeue:i'`. Length = to the lists in batch_spec. float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Batch.inputs: 0>: <tf.Tensor 'fifo_queue_Dequeue:0' shape=(1, ?, ?, 3) dtype=float32>,\n",
       " <Batch.part_score_targets: 1>: <tf.Tensor 'fifo_queue_Dequeue:1' shape=(1, ?, ?, 4) dtype=float32>,\n",
       " <Batch.part_score_weights: 2>: <tf.Tensor 'fifo_queue_Dequeue:2' shape=(1, ?, ?, 4) dtype=float32>,\n",
       " <Batch.locref_targets: 3>: <tf.Tensor 'fifo_queue_Dequeue:3' shape=(1, ?, ?, 8) dtype=float32>,\n",
       " <Batch.locref_mask: 4>: <tf.Tensor 'fifo_queue_Dequeue:4' shape=(1, ?, ?, 8) dtype=float32>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**enqueue_op** is a tf.Operation object:\n",
    "`<tf.Operation 'fifo_queue_1_enqueue' type=QueueEnqueueV2>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**placeholders** is a dictionary like **batch** storing tensors of class a `tf.Tensor 'Placeholder_5:0'`. Length the same and float 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Batch.inputs: 0>: <tf.Tensor 'Placeholder:0' shape=(1, ?, ?, 3) dtype=float32>,\n",
       " <Batch.part_score_targets: 1>: <tf.Tensor 'Placeholder_1:0' shape=(1, ?, ?, 4) dtype=float32>,\n",
       " <Batch.part_score_weights: 2>: <tf.Tensor 'Placeholder_2:0' shape=(1, ?, ?, 4) dtype=float32>,\n",
       " <Batch.locref_targets: 3>: <tf.Tensor 'Placeholder_3:0' shape=(1, ?, ?, 8) dtype=float32>,\n",
       " <Batch.locref_mask: 4>: <tf.Tensor 'Placeholder_4:0' shape=(1, ?, ?, 8) dtype=float32>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up: batch, batch_spec and placeholders are dictionaries with 5 entries. Every key is a Batch object (Enum class). The 5 keys are shared by all 3 dictionaries.\n",
    "* batch stores **TF queue_Dequeue tensors**\n",
    "* batch_spec stores **Python lists**\n",
    "* placeholders stores **TF Placeholder tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the 5 keys??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is losses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses is a dict with three different losses, each a tensor\n",
    "1. **part_loss**: `<tf.Tensor 'sigmoid_cross_entropy_loss/value:0' shape=() dtype=float32>`\n",
    "2. **locref_loss**: `<tf.Tensor 'mul:0' shape=() dtype=float32>`\n",
    "3. **total_loss**: `<tf.Tensor 'add:0' shape=() dtype=float32>`\n",
    "\n",
    "It's the output of the `train` method in the PoseNet class.\n",
    "  * `PoseNet.train(batch)\n",
    "  \n",
    "Their value is tracked with the `summary.scalars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fifo_queue_Dequeue:2\", shape=(1, ?, ?, 4), dtype=float32)\n",
      "dict_keys([<Batch.inputs: 0>, <Batch.part_score_targets: 1>, <Batch.part_score_weights: 2>, <Batch.locref_targets: 3>, <Batch.locref_mask: 4>])\n"
     ]
    }
   ],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.dataset.pose_dataset import Batch\n",
    "print(batch[Batch.part_score_weights])\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.nnet.net_factory import pose_net\n",
    "losses = pose_net(cfg).train(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking deeper into the pose_net(cfg).train(batch) statement\n",
    "This statement does the following:\n",
    "\n",
    "* Instantiate an object of class PoseNet taking the cfg file as input. cfg is assigned to self\n",
    "* Then, the train method in the object is called with batch as input\n",
    "\n",
    "**The train() method** runs the get_net() method with the value under Batch.input key in the batch dictionary. This gives us heads, which is a dictionary with 2 keys, `part_pred` and `locref` (1). The values are `slim.conv2d_transpose` objects, the output of `prediction_layer`. This objects store a \n",
    "\n",
    "1. `part_pred_interm` is a third key available only if `location_refinement` is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def train(self, batch):\n",
    "        cfg = self.cfg\n",
    "\n",
    "        heads = self.get_net(batch[Batch.inputs])\n",
    "\n",
    "        weigh_part_predictions = cfg.weigh_part_predictions\n",
    "        part_score_weights = batch[Batch.part_score_weights] if weigh_part_predictions else 1.0\n",
    "\n",
    "        def add_part_loss(pred_layer):\n",
    "            return tf.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets],\n",
    "                                                   heads[pred_layer],\n",
    "                                                   part_score_weights)\n",
    "\n",
    "        loss = {}\n",
    "        loss['part_loss'] = add_part_loss('part_pred')\n",
    "        total_loss = loss['part_loss']\n",
    "        if cfg.intermediate_supervision:\n",
    "            loss['part_loss_interm'] = add_part_loss('part_pred_interm')\n",
    "            total_loss = total_loss + loss['part_loss_interm']\n",
    "\n",
    "        if cfg.location_refinement:\n",
    "            locref_pred = heads['locref']\n",
    "            locref_targets = batch[Batch.locref_targets]\n",
    "            locref_weights = batch[Batch.locref_mask]\n",
    "\n",
    "            loss_func = losses.huber_loss if cfg.locref_huber_loss else tf.losses.mean_squared_error\n",
    "            loss['locref_loss'] = cfg.locref_loss_weight * loss_func(locref_targets, locref_pred, locref_weights)\n",
    "            total_loss = total_loss + loss['locref_loss']\n",
    "\n",
    "        # loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize)\n",
    "        loss['total_loss'] = total_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " This is summarised by:  * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_layer(cfg, input, name, num_outputs):\n",
    "    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding='SAME',\n",
    "                        activation_fn=None, normalizer_fn=None,\n",
    "                        weights_regularizer=slim.l2_regularizer(cfg.weight_decay)):\n",
    "        with tf.variable_scope(name):\n",
    "            pred = slim.conv2d_transpose(input, num_outputs,\n",
    "                                         kernel_size=[3, 3], stride=2,\n",
    "                                         scope='block4')\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_layers(self, features, end_points, reuse=None):\n",
    "    cfg = self.cfg\n",
    "\n",
    "    num_layers = re.findall(\"resnet_([0-9]*)\", cfg.net_type)[0]\n",
    "    layer_name = 'resnet_v1_{}'.format(num_layers) + '/block{}/unit_{}/bottleneck_v1'\n",
    "\n",
    "    out = {}\n",
    "    with tf.variable_scope('pose', reuse=reuse):\n",
    "        out['part_pred'] = prediction_layer(cfg, features, 'part_pred',\n",
    "                                            cfg.num_joints)\n",
    "        if cfg.location_refinement:\n",
    "            out['locref'] = prediction_layer(cfg, features, 'locref_pred',\n",
    "                                             cfg.num_joints * 2)\n",
    "        if cfg.intermediate_supervision:\n",
    "            interm_name = layer_name.format(3, cfg.intermediate_supervision_layer)\n",
    "            block_interm_out = end_points[interm_name]\n",
    "            out['part_pred_interm'] = prediction_layer(cfg, block_interm_out,\n",
    "                                                       'intermediate_supervision',\n",
    "                                                       cfg.num_joints)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.slim.nets import resnet_v1\n",
    "from deeplabcut.pose_estimation_tensorflow.nnet import losses\n",
    "\n",
    "\n",
    "# net_funcs = {'resnet_50': resnet_v1.resnet_v1_50,\n",
    "#              'resnet_101': resnet_v1.resnet_v1_101}\n",
    "# class PoseNet:\n",
    "#     def __init__(self, cfg):\n",
    "#         self.cfg = cfg\n",
    "#\n",
    "#     def extract_features(self, inputs):\n",
    "#         net_fun = net_funcs[self.cfg.net_type]\n",
    "\n",
    "#         mean = tf.constant(self.cfg.mean_pixel,\n",
    "#                            dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "#         im_centered = inputs - mean\n",
    "\n",
    "#         # The next part of the code depends upon which tensorflow version you have.\n",
    "#         vers = tf.__version__\n",
    "#         vers = vers.split(\".\") #Updated based on https://github.com/AlexEMG/DeepLabCut/issues/44\n",
    "#         if int(vers[0])==1 and int(vers[1])<4: #check if lower than version 1.4.\n",
    "#             with slim.arg_scope(resnet_v1.resnet_arg_scope(False)):\n",
    "#                 net, end_points = net_fun(im_centered,\n",
    "#                                           global_pool=False, output_stride=16)\n",
    "#         else:\n",
    "#             with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "#                 net, end_points = net_fun(im_centered,\n",
    "#                                           global_pool=False, output_stride=16,is_training=False)\n",
    "\n",
    "#         return net,end_points\n",
    "\n",
    "#\n",
    "#     def get_net(self, inputs):\n",
    "#         net, end_points = self.extract_features(inputs)\n",
    "#         return self.prediction_layers(net, end_points)\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, t in losses.items():\n",
    "    tf.summary.scalar(k, t)\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "total_loss = losses[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Variables to restore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "variables_to_restore = slim.get_variables_to_restore(include=[\"resnet_v1\"])\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "saver = tf.train.Saver(max_to_keep=max_to_keep)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge amount of variables extracted from the pretrained ResNet50 print(len(variables_to_restore))\n",
    "TODO: Google restorers and savers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.saver.Saver at 0x7f78a2d79438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from deeplabcut.pose_estimation_tensorflow.train import load_and_enqueue\n",
    "# def load_and_enqueue(sess, enqueue_op, coord, dataset, placeholders):\n",
    "#     while not coord.should_stop():\n",
    "#         batch_np = dataset.next_batch()\n",
    "#         food = {pl: batch_np[name] for (name, pl) in placeholders.items()}\n",
    "#         sess.run(enqueue_op, feed_dict=food)\n",
    "\n",
    "        \n",
    "from deeplabcut.pose_estimation_tensorflow.train import start_preloading\n",
    "# def start_preloading(sess, enqueue_op, dataset, placeholders):\n",
    "#     coord = tf.train.Coordinator()\n",
    "\n",
    "#     t = threading.Thread(target=load_and_enqueue,\n",
    "#                          args=(sess, enqueue_op, coord, dataset, placeholders))\n",
    "#     t.start()\n",
    "\n",
    "#     return coord, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "coord, thread = start_preloading(sess, enqueue_op, dataset, placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.train import get_optimizer\n",
    "# def get_optimizer(loss_op, cfg):\n",
    "#     learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "#     if cfg.optimizer == \"sgd\":\n",
    "#         optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "#     elif cfg.optimizer == \"adam\":\n",
    "#         optimizer = tf.train.AdamOptimizer(cfg.adam_lr)\n",
    "#     else:\n",
    "#         raise ValueError('unknown optimizer {}'.format(cfg.optimizer))\n",
    "#     train_op = slim.learning.create_train_op(loss_op, optimizer)\n",
    "\n",
    "#     return learning_rate, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(cfg.log_dir, sess.graph)\n",
    "learning_rate, train_op = get_optimizer(total_loss, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from /home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_iters overwritten as 100\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 100\n"
     ]
    }
   ],
   "source": [
    "# Restore variables from disk.\n",
    "restorer.restore(sess, cfg.init_weights)\n",
    "if maxiters==None:\n",
    "    max_iter = int(cfg.multi_step[-1][1])\n",
    "else:\n",
    "    max_iter = min(int(cfg.multi_step[-1][1]),int(maxiters))\n",
    "    #display_iters = max(1,int(displayiters))\n",
    "    print(\"Max_iters overwritten as\",max_iter)\n",
    "\n",
    "if displayiters==None:\n",
    "    display_iters = max(1,int(cfg.display_iters))\n",
    "else:\n",
    "    display_iters = max(1,int(displayiters))\n",
    "    print(\"Display_iters overwritten as\",display_iters)\n",
    "\n",
    "if saveiters==None:\n",
    "     save_iters=max(1,int(cfg.save_iters))\n",
    "\n",
    "else:\n",
    "    save_iters=max(1,int(saveiters))\n",
    "    print(\"Save_iters overwritten as\",save_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_tensorflow.train import LearningRate\n",
    "\n",
    "# class LearningRate(object):\n",
    "#     def __init__(self, cfg):\n",
    "#         self.steps = cfg.multi_step\n",
    "#         self.current_step = 0\n",
    "\n",
    "#     def get_lr(self, iteration):\n",
    "#         lr = self.steps[self.current_step][0]\n",
    "#         if iteration == self.steps[self.current_step][1]:\n",
    "#             self.current_step += 1\n",
    "\n",
    "#         return lr\n",
    "\n",
    "    \n",
    "cum_loss = 0.0\n",
    "lr_gen = LearningRate(cfg)\n",
    "stats_path = Path(config_yaml).with_name('learning_stats.csv')\n",
    "lrf = open(str(stats_path), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deeplabcut.pose_estimation_tensorflow.train.LearningRate at 0x7f78704a5908>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples/Reaching-Mackenzie-2018-08-30/dlc-models/iteration-0/ReachingAug30-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'dataset_type': 'default', 'use_gt_segm': False, 'batch_size': 1, 'video': False, 'video_batch': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['Hand', 'Finger1', 'Finger2', 'Joystick'], 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ReachingAug30/Reaching_Mackenzie95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/home/antortjim/anaconda3/envs/DLC/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_ReachingAug30/Documentation_data-Reaching_95shuffle1.pickle', 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples/Reaching-Mackenzie-2018-08-30', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25}\n",
      "Starting training....\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.0553 lr: 0.005\n"
     ]
    }
   ],
   "source": [
    "print(\"Training parameter:\")\n",
    "print(cfg)\n",
    "print(\"Starting training....\")\n",
    "for it in range(max_iter+1):\n",
    "    print(it)\n",
    "    current_lr = lr_gen.get_lr(it)\n",
    "    [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n",
    "                                      feed_dict={learning_rate: current_lr})\n",
    "    cum_loss += loss_val\n",
    "    train_writer.add_summary(summary, it)\n",
    "  \n",
    "    if it % display_iters == 0 and it>0:\n",
    "        average_loss = cum_loss / display_iters\n",
    "        cum_loss = 0.0\n",
    "        logging.info(\"iteration: {} loss: {} lr: {}\"\n",
    "                     .format(it, \"{0:.4f}\".format(average_loss), current_lr))\n",
    "        lrf.write(\"{}, {:.5f}, {}\\n\".format(it, average_loss, current_lr))\n",
    "        lrf.flush()\n",
    "\n",
    "    # Save snapshot\n",
    "    if (it % save_iters == 0 and it != 0) or it == max_iter:\n",
    "        model_name = cfg.snapshot_prefix\n",
    "        saver.save(sess, model_name, global_step=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrf.close()\n",
    "sess.close()\n",
    "coord.request_stop()\n",
    "coord.join([thread])\n",
    "#return to original path.\n",
    "os.chdir(str(start_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/antortjim/MEGA/FlySleepLab/DeepLabCut/examples'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
